{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import ffmpeg\n",
    "import tempfile\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(paths) -> dict:\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    audio_paths= {}\n",
    "    for path in paths:\n",
    "        filename = os.path.basename(path).split('.')[0]\n",
    "        print(f\"Extracting audio from {os.path.basename(path)}...\")\n",
    "        output_path = os.path.join(temp_dir, f\"{filename}.wav\")\n",
    "\n",
    "        ffmpeg.input(path).output(\n",
    "            output_path,\n",
    "            acodec=\"pcm_s16le\", ac=1, ar=\"16k\"\n",
    "        ).run(quiet=True, overwrite_output=True)\n",
    "        audio_paths[path] = output_path\n",
    "    return audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_transcript(audio_path, text_path, transcribe: callable):\n",
    "    print(f\"Generating transcript for {os.path.basename(audio_path)} audio... This might take a while.\")\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    result = transcribe(audio_path)\n",
    "    warnings.filterwarnings(\"default\")\n",
    "\n",
    "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result[\"text\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(audio_paths: list, output_text: bool, output_dir: str, transcribe: callable):\n",
    "    text_path = output_dir if output_text else tempfile.gettempdir()\n",
    "    for path, audio_path in audio_paths.items():\n",
    "        filename = os.path.basename(path).split('.')[0]\n",
    "        text_path = os.path.join(text_path, f\"{filename}.txt\")\n",
    "\n",
    "        result = write_transcript(audio_path, text_path, transcribe)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_stt(video_path:str, model:str, output_dir:str, srt:bool, verbose:bool):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if model.endswith(\".en\"):\n",
    "        print(f\"{model} is a English model\")\n",
    "    model = whisper.load_model(model)\n",
    "    audio = get_audio(video_path)\n",
    "    subtitle = get_transcript(\n",
    "                audio, srt, output_dir, lambda audio_path: model.transcribe(audio_path, \n",
    "                                                verbose=verbose, task='transcribe'))\n",
    "    return subtitle    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLProject6\\Video_Summarizer\\vid\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from sample.mp4...\n",
      "Generating transcript for sample.wav audio... This might take a while.\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15257/15257 [00:09<00:00, 1666.25frames/s]\n"
     ]
    }
   ],
   "source": [
    "video_path = [r\"D:\\MLProject6\\Video_Summarizer\\uploads\\sample.mp4\"]\n",
    "\n",
    "transcript = initiate_stt(video_path=video_path, model='tiny', output_dir='transcript_text', srt=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" If we have a one minute speech to deliver, the main challenge that we face is how to be very consolidated so much important information into just one minute. And more importantly, how do we communicate this in a clear manner? That's where this new clicking communication thing work called, brek, comes into the picture. This framework will not only help us speak in a concise manner, why making sure all the important points are in place, but the best thing about it is that if we have one minute speeches or short speeches to give in almost any occasion, this framework can be applied there. So it also improves our ability to speak on this part, even when we're not prepared. So what does this framework stand for? brek, it's an abbreviation with stands for point, reason, example point. We basically start off with the main point or message of our speech, then we want to give the reason for that main point. Then we give an example and finally end it again with our main point. Let's look at an example. Let's say we have to give a simple speech around our favorite genre of music. Now one of two things can happen. Either a weekend look at it in a way that there's so much to speak about under the topic of music, but on the other end we might also think how do I speak beyond just my normal answer. For instance, a far-fabric type of music is jazz, but we say beyond high my favorite genre of music is jazz. For both of these instances, brek can be very, very useful. Let's get into the example. My favorite type of music is rock and roll. The reason I like rock and roll music is to start with this into it ever since I was a little kid. And I absolutely love the heavy guitar and the drumrolls and the lyrics of rock songs. But more than any of this, I remember attending my first ever rock, called it so. That atmosphere was electrifying and I remember it to stay. It immediately made me fall in love with the genre. My favorite rock man is this one group called Lincoln Park. They were one of the first bands that I listened to and I loved everything about them. From their melodies to their rhythms to their lyrics. Unfortunately, I never got to seek them perform live, but I still listen to their music at three days. And that is why rock and roll is my favorite type of music. And that's basically this framework can be applied to a most any question or any short speech that they need to give. And if you want to know more such frameworks that you can apply to short speeches, be made an entire video on delivering memorable ones speeches, which has a lot more than just frameworks. And I highly recommend you check that out right here.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLProject6\\Video_Summarizer\\vid\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\MLProject6\\Video_Summarizer\\vid\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sakku\\.cache\\huggingface\\hub\\models--philschmid--bart-large-cnn-samsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(transcript['text'], max_length=512, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In brek, the main point or message of a speech is followed by the reason for that main point, followed by an example, and then the last point. The framework can be applied to a variety of topics, such as the topic of favorite music or short speeches.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=512)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
